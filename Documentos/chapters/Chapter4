\chapter*{Modelo Computacional SIMD}\label{Modelo Computacional SIMD}
\addcontentsline{toc}{chapter}{Modelo Computacional SIMD} % si queremos que aparezca en el 
\markboth{}{} % encabezado

En esta secci\'on explicaremos el uso  GPU en el \'ambito de la programaci\'on de alto desempe\~no (HPC). Daremos las razones que motivaron su uso y como fueron evolucionando las diferentes arquitecturas.


\section{Introducci\'on}

Todos usamos la computadora, pero muchas veces no somos conscientes de la tecnolog\'ia que hay en ellos. 
M\'aquinas incre\'ibles que nos permiten disfrutar del ocio con videojuegos de gr\'aficos muy cercanos a la realidad. 
Para estas tareas el trabajo de la GPU (o placas de video) es fundamental.

Hoy nos adentraremos en el mundo de las placas gr\'aficas, de la arquitecturas de las GPU y de sus diferencias con otro procesador, el CPU. 
Porque las tarjetas gr\'aficas son esenciales en la inform\'atica actual y son parte b\'asica de lo que para muchos es importante de cara al futuro: 
GPGPU, o el procesamiento de datos mediante GPU.

\section{La era pre-GPU}

Las cosas han cambiado mucho desde que las computadoras empezaron a implantarse en nuestros hogares, all\'a por la d\'ecada de los 80. El hardware se sustenta sobre las mismas bases de la arquitectura von Neumann, si bien \'esta ha evolucionado de forma muy notable y los sistemas actuales son ahora mucho m\'as complejos.

El tr\'io de componentes que plante\'o John von Neumann eran tres: ALU, memoria y entrada/salida, refiri\'endose a mecanismos que procesan, almacenan y reciben/env\'ian la informaci\'on, respectivamente. Interpretando la arquitectura en un equipo actual equivaldr\'ia a tener s\'olo un procesador, un disco, un teclado y una pantalla. 
Evidentemente un sistema moderno est\'a formado por muchos m\'as elementos, y entre ellos la GPU se ha convertido en uno de los actuales componentes fundamentales.

\section{Los \'origenes}

En los primeros ordenadores el procesador central -- CPU, central processing unit -- era el encargado de gestionar y procesar todo tipo de informaci\'on. Desde los datos que el usuario quer\'ia operar hasta por supuesto el sistema operativo, y con \'el su interfaz.

Si bien aquellos primeros sistemas utilizaban interfaces basadas en texto, con la llegada de las primeras interfaces gr\'aficas el nivel de exigencia creci\'o no s\'olo en el propio sistema operativo, si no tambi\'en en muchas de las aplicaciones que empezaban a surgir por la \'epoca. Programas CAD o videojuegos, por ejemplo, requer\'ian muchos m\'as recursos para funcionar correctamente.

Llegados a este punto, los dise\~nadores de sistemas se basaron en un componente que ya exist\'ia para evolucionarlo y hacerlo crecer. Los coprocesadores matem\'aticos o FPU -- floating-point unit -- eran utilizados en muchos sistemas para acelerar el procesamiento de datos. Pueden entenderse como un segundo procesador, si bien algunas de las diferencias respecto de las CPU son muy claras: no pueden tener acceso a los datos directamente o ejecutan un juego de instrucciones mucho m\'as sencillo pensado para tratar datos en flotante.

Las exigencias continuaron creciendo, y los sistemas de la \'epoca dispon\'ian de CPU y una FPU optativa que termin\'o convirti\'endose en fundamental: los coprocesadores matem\'aticos evolucionaron hacia las GPU, al ser el componente m\'as eficiente a la hora de procesar y determinar el aspecto gr\'afico de todo tipo de software.

\section{M\'as unidades de proceso que CPU, pero m\'as sencillas: la arquitectura de las GPU}

Los coprocesadores primero y las tarjetas gr\'aficas despu\'es plantearon una arquitectura hardware muy diferente de las utilizadas en las CPU.

En una GPU tambi\'en tendremos unidades de proceso (tradicionalmente Unified Shaders o Stream Processors), memoria (que hace las veces de memoria RAM, como un sistema de almacenamiento temporal para apoyar al procesamiento de los datos) y entrada/salida. Las GPU tambi\'en tienen sus propias bibliotecas para que los desarrolladores programen el software.

Estos Unified Shaders son los que procesan la informaci\'on gr\'afica, y a priori cuantos m\'as tengamos ser\'a mejor para la capacidad de proceso de la GPU. Este factor es importante en videojuegos, pero fundamental para uno de los usos que m\'as est\'an creciendo en los \'ultimos a\~nos: GPGPU, General-purpose computing on graphics processing units.

\section{GPGPU presente}

Los sistemas crecen, pero lo hacen debido a lo que tanto los desarrolladores como los usuarios finales van exigiendo. Los videojuegos y su calidad gr\'afica son la prueba m\'as palpable de este incremento en las capacidades de proceso, pero el uso que le damos a las GPU tambi\'en ha cambiado para adaptarse a otras posibilidades.

GPGPU es el t\'ermino que se utiliza para designar las tareas de prop\'osito general, t\'ipicamente pensadas para ser procesadas en una CPU, pero que se aprovechan del potencial de la GPU para ejecutarse en ella. Dado que los procesadores gr\'aficos son mucho m\'as eficientes en cierto tipo de operaciones, los resultados se obtendr\'an m\'as r\'apidamente.

El problema de la GPGPU es precisamente que no todas las tareas tienen que ser m\'as eficientes en una GPU. \'Estas est\'an especializadas en tareas altamente paralelizables cuyos algoritmos puedan subdividirse, procesarse por separado para luego unir los subresultados y tener el resultado final.


\section{CUDA como modelos de programaci\'on escalable}

Las arquitecturas de CPU de m\'ultiples n\'ucleos y GPU significaron que los chips 
de procesadores convencionales fueran sistemas paralelos. M\'as a\'un, su paralelismo
continua escalando con la ley de Moore. El desaf\'io es lograr construir aplicaciones
que utilicen este paralelismo y que de forma transparente escalen para aprovechar
el incremento en el n\'umero de n\'ucleos. Tal como las aplicaciones de procesamiento
gr\'afico 3D escalan su paralelismo a GPU de m\'ultiples n\'ucleos.

El modelo de programaci\'on paralelo de CUDA est\'a dise\~nado para sobreponerse a este
desaf\'io mientras facilita el aprendizaje con la utilizaci\'on del est\'andar de C.

Su modelo proporciona tres tipos de abstracciones: una jerarqu\'ia de grupos de hilos (threads),
memoria compartida y barreras de sincronizaci\'on. Estas son utilizadas por el programador
a trav\'es de un n\'umero peque\~no extensiones del lenguaje C.

Estas abstracciones proveen de paralelismo de datos de grano-fino y paralelismo de
hilos, mezclado en medio de un paralelismo de datos de grano-grueso y paralelismo de tareas.
Esto lleva al usuario a dividir el problema en subproblemas que puedan ser solucionados
independientemente en paralelo por bloques de hilos, y cada problema en partes m\'as
peque\~nas que puedan ser resueltos de forma cooperativa en paralelo por todas los
hilos de un mismo bloque.

Esta descomposici\'on preserva la expresividad del lenguaje, permitiendo a los hilos
cooperar cuando solucionan cada subproblema, y al mismo tiempo permite la escalabilidad
autom\'aticamente. De este modo, cada bloque de hilos puede ser asignado a cualquiera
de los multiprocesadores disponibles en la GPU, en cualquier orden, de forma concurrente
o secuencial, permitiendo que el c\'odigo CUDA pueda ejecutarse en cualquier n\'umero
de multiprocesadores y solo el sistema de planificaci\'on debe conocer la cantidad
f\'isica de multiprocesadores, como se ilustra en la Figura \ref{CUDA1}

\begin{figure}[!htbp]
  \begin{center}
    \leavevmode

    \includegraphics[]{automatic-scalability.png}

    \caption{Escalabilidad Autom\'atica}
    \label{CUDA1}
  \end{center}
\end{figure}


\subsection{Modelo de programaci\'on}

En esta Secci\'on presentaremos los conceptos principales el modelo de programaci\'on
de CUDA C. CUDA C extiende el lenguaje est\'andar C, permitiendo al programador definir
funciones, llamadas \textit{Kernels}, que cuando son llamadas se ejecutan $N$ veces
en paralelo por $N$ hilos de CUDA, a diferencia de solo un hilo en una funci\'on regular
de C. El programador es qui\'en decide el valor din\'amico o est\'atico del par\'ametro $N$
en el momento de ejecutar el kernel. A cada hilo que ejecuta un kernel se le asigna
un identificador \'unico el cual es accesible por el hilo dentro del kernel. Estos identificadores
siguen los lineamientos de la jerarqu\'ia de hilos analizada a continuaci\'on.

\subsection*{Jerarqu\'ia de Hilos}

Cada identificador de hilo puede ser visto como una 3-upla, por lo que cada hilo
puede ser identificado utilizando un \'indice de una, dos o tres dimensiones. Formando as\'i
un bloque de hilos de una dos o tres dimensiones. Esto provee una forma natural de
mapear los identificadores de hilos con el accesos a datos. Hay un l\'imite en el 
n\'umero de hilos por bloques, ya que se espera que cada bloque de hilos resida en 
un mismo multiprocesador y debe compartir recursos de memoria limitados dentro
del procesador. El n\'umero m\'aximo de hilos por bloques es de 1024 en la arquitectura
utilizada en este trabajo.

Los bloques son organizados en grillas de una, dos o tres dimensiones como se ilustra
en la Figura \ref{CUDA2}. El n\'umero de bloques de hilos en una grilla est\'a normalmente
limitado directamente por le tama\~no de los datos a procesar o el n\'umero de procesadores
en el sistema.

\begin{figure}[!htbp]
  \begin{center}
    \leavevmode

    \includegraphics[]{grid-of-thread-blocks.png}

    \caption{Grilla de bloques de hilos}
    \label{CUDA2}
  \end{center}
\end{figure}

Cada bloque dentro de una grilla puede ser identificado por un \'indice de una,
dos o tres dimensiones seg\'un haya sido declarado y accesible dentro del kernel
a trav\'es de una variable predefinida. Del mismo modo es accesible las dimensiones
del bloque de hilos y de la grilla de bloques. 

Los hilos dentro de un bloque pueden cooperar entre ellos compartiendo datos
a trav\'es de memoria compartida y sincronizando su ejecuci\'on para coordinar el accesos
a esta memoria. Para que la cooperaci\'on sea eficiente, se requiere que el acceso a memoria
compartida tenga baja latencia y la sincronizaci\'on no tenga una gran penalizaci\'on.

\subsection*{Jerarqu\'ia de Memoria}

Los hilos de CUDA pueden acceder a diferentes espacios de memoria durante su ejecuci\'on como
se ilustra en la Figura \ref{CUDA3}. Cada hilo dispone de memoria local. Cada bloque de hilos
dispone de memoria compartida visible por todos los hilos de un mismo bloque. Todos
los hilos tiene acceso a la misma memoria global.

Hay adicionalmente dos memorias de solo lectura accesible por todos los hilos: memoria
constante y memoria de textura. La memoria global, memoria de textura y memoria
constante est\'an optimizadas para diferentes usos. La memoria de textura ofrece un
modo de acceso y filtrado de datos para formatos de memoria espec\'ificos. No cubriremos
este tipo de memoria ya que no es utilizada en el trabajo.

\begin{figure}[!htbp]
  \begin{center}
    \leavevmode

    \includegraphics[]{memory-hierarchy.png}

    \caption{Jerarqu\'ia de memoria}
    \label{CUDA3}
  \end{center}
\end{figure}


\subsection*{Programaci\'on Heterog\'enea}

El modelo de programaci\'on de CUDA asume que los hilos de CUDA ejecutan en un dispositivo
f\'isicamente separado que opera como un coprocesador del \textit{host} que esta ejecutando
la aplicaci\'on, por lo general y para el an\'alisis de nuestro trabajo esta aplicaci\'on 
est\'a escrita en C/C++ utilizando el lenguaje CUDA. El c\'odigo kernel ser\'a ejecutado
espec\'ificamente en la GPU y el resto del programa se ejecutar\'a en el procesador
central o CPU.

Adem\'as, el modelo computacional de CUDA asume que tanto el host como el dispositivo
manejan distintos espacios de memoria, referidos como \textit{memoria de host}
y \textit{memoria de dispositivo} respectivamente. CUDA provee una API completa
para manejar la memoria de dispositivo y poder ser alocada, escrita y le\'ida por 
el host.



\section{Implementaci\'on del hardware CUDA GPGPU}

La arquitectura de NVIDA GPU est\'a construida alrededor de un arreglo de
procesadores de flujo de m\'ultiples hilos o \textit{Streaming Multiprocessors (SMs)}.
Cuando un programa CUDA est\'a ejecutando en host e invoca la ejecuci\'on de una grilla
de kernels, los bloques de la grilla son numerados y distribuidos a los multiprocesadores
disponibles para su ejecuci\'on. Los hilos de un bloque ejecutan concurrentemente en
un mismo multiprocesador, y m\'ultiples bloques de hilos pueden ejecutar de forma
concurrente en un mismo multiprocesador. A medida que los bloques de hilos terminan,
nuevos bloques son asignados a los multiprocesadores vacantes.

Los multiprocesadores est\'an dise\~nados para ejecutar cientos de hilos de forma concurrente. 
Para manejar este n\'umero de hilos, estos utilizan una arquitectura llamada 
\textit{SIMT (Single Instruction, Multiple Thread)}


\subsection{Arquitectura SIMT}

Los multiprocesadores crean, manejan, planifican y ejecutan en paralelo grupos de 32 hilos,
llamados \textit{warps}. Cada hilo de un warp comienzan juntas en el mismo punto del
programa, pero cada uno tiene su propio contador de instrucciones y registros de estados
y son libres de ejecutar independientemente.

Cuando un multiprocesador posee uno o m\'as bloques de hilos para ejecutar, este
parte los bloques en warps y cada warp es planificado por un planificador de warps para ser ejecutada. 
La forma en que los bloques son divididos en warps es siempre la misma; cada bloque contiene
hilos con identificadores num\'ericos asignados de forma consecutiva. El primer warp contiene
los hilos con identificadores 0 a 31, la segunda warp los hilos 32 a 63 y as\'i sucesivamente.

Cada hilo dentro de un mismo warp ejecuta una misma instrucci\'on al mismo tiempo, por lo tanto el rendimiento
\'optimo se consigue cuando los 32 hilos de un warp siguen el mismo camino de ejecuci\'on.
Si los hilos de un warp divergen en el flujo de ejecuci\'on, las ejecuci\'on de los 
hilos del warp son serializados, deshabilitando los hilos que no est\'an en el flujo
de ejecuci\'on y cuando los posibles caminos convergen, todos los hilos vuelven
al mismo punto del programa. Esto solo ocurre entre hilos de un mismo warp. Diferentes
warp ejecutan independientemente.

Si analizamos la correcci\'on del programa, el programador puede esencialmente
ignorar el comportamiento de la arquitectura SIMT. Sin embargo, se pueden conseguir
sustanciales resultados de mejoramiento de rendimiento teniendo en cuenta la forma
en que los hilos son agrupados y como es el comportamiento de ellos en los warps.
En la pr\'actica, esto es an\'alogo a como se comporta la cache. El tama\~no de cache
puede ser ignorado en la correcci\'on del dise\~no, pero debe ser considerado en la
estructura del c\'odigo para conseguir el rendimiento m\'aximo. La arquitectura SIMT,
requiere de ciertos cuidados al acceder a la memoria y manejar la divergencia
de los hilos, estos ser\'an analizados a m\'as adelante.

\subsection{Caracter\'isticas del Hardware Multi-hilo}

El contexto de ejecuci\'on de cada warp (contadores de programa, registros, etc.) es mantenido
en la memoria interna de cada multiprocesador lo largo de la vida del warp. Esto implica que cambiar
de un contexto de ejecuci\'on a otro no tiene costo. Esto es aprovechado para
que los multiprocesadores mantengan un conjunto de warps activas para la ejecuci\'on
y el planificador de ejecuci\'on del multiprocesador elije cual es la
siguiente warp a ejecutar. El modo de manejar la ejecuci\'on de las warps es una 
gran ventaja en el dise\~no de la arquitectura, permitiendo ocultar de forma 
\'optima la latencia de lectura y escritura a memoria, siempre y cuando el 
multiprocesador tenga suficientes warps disponibles para la ejecuci\'on.

En particular, cada multiprocesador contiene una conjunto de registros de 32-bits
que son divididos a lo largo de los warps y una cache de datos y memoria compartida
que es dividida a lo largo de los bloques de hilos. El n\'umero de bloques y warps 
que pueden residir y ser procesados al mismo tiempo dentro de un multiprocesador 
para un programa dado, depende de la cantidad de
registros y memoria compartida utilizada para el programa y la cantidad
de registros y memoria compartida disponibles en el multiprocesador. Tambi\'en
existe un n\'umero m\'aximo de bloques residentes y de warps residentes en cada
multiprocesador.

Para comprender este hecho, veremos un ejemplo concreto en la arquitectura
espec\'ifica utilizada en el trabajo. Hablamos de la arquitectura Kepler de NVIDA.
En esta arquitectura, el tama\~no de los warps es 32 y cada multiprocesador posee
256 KB de memoria de registros y memoria compartida programable en 16, 32 o 48 KB.
Suponemos que poseemos un kernel que utiliza 25 registros locales de 32 bit y 
cada bloque lanzado es de 256 hilos. Cada bloque
necesita de $256 \times 25 \times 4 = 25 KB$ lo cual nos indica que no puede haber
m\'as de 10 bloques simult\'aneamente en el mismo SM, de haberlo el multiprocesador se
quedar\'ia sin memoria local. Recordemos que cada hilo necesita que sus valores locales
persistan en memoria local a lo largo de su ejecuci\'on para permitir que el planificador
los saque y ponga en ejecuci\'on r\'apidamente. Del mismo modo si los SM est\'an configurados
para tener 48KB de memoria compartida y cada bloque utiliza 12KB de esta memoria,
no puede haber mas de 4 bloques simult\'aneamente en el mismo SM. De estos dos
par\'ametros analizados para determinar, en tiempo de compilaci\'on, cuantos bloques
pueden residir el cada SM, el m\'inimo entre ambos ser\'a el valor final.

De lo analizado anteriormente se desprende un valor de utilizaci\'on de los multiprocesadores
o \textit{occupancy} que es la porcentaje entre la cantidad de bloques de un kernel
en particular que puede manejar cada cada multiprocesador y la cantidad m\'axima de
bloques determinados por la arquitectura. En el caso de la arquitectura Kepler el n\'umero m\'aximo
de bloques por SM es 16. As\'i, Occupancy es un valor entre 0 y 1. Mientras m\'as cerca de 1 se encuentre, no significar\'a
que el c\'odigo ser\'a m\'as eficiente ya que esto depende de la combinaci\'on de muchos
factores, pero determina cuan ocupado estar\'an los SM, permitiendo as\'i mejorar
el ocultamiento de latencia de accesos a memoria entre otras cosas.


\section{T\'ecnicas de Rendimiento}

Para lograr conseguir el m\'aximo de rendimiento de la arquitectura GPU es necesario
adaptar el problema para seguir algunos lineamentos de la arquitectura. En nuestro
problema trataremos de conseguir :

\begin{enumerate}
\item Maximizar la ejecuci\'on en paralelo para alcanzar la m\'axima utilizaci\'on.
\item Optimizar el uso de la memoria para alcanzar el m\'aximo ancho de banda.
\end{enumerate}

Para lograr la m\'axima utilizaci\'on debemos separar el problema en bloques lo m\'as
independientes posibles para que estos puedan ser mapeados a diferentes componentes
del sistema y mantener estos componentes lo m\'as ocupados posible. A nivel multiprocesador,
como ya explicamos, es importante que haya muchas warps activas
dispuestas a ejecutar para poder ocultar la latencia de acceso a memoria. Adem\'as,
es necesario que las threads de un mismo warp minimicen las bifurcaciones y las sincronizaci\'on
como barreras o mutex de escritura de memoria.

En cuanto a utilizaci\'on de memoria, el primer paso es tratar de maximizar el rendimiento
en el accesos a memoria de bajo ancho de banda, es decir, memoria que reside en el dispositivo.
Las t\'ecnicas m\'as utilizadas son dise\~nar el algoritmo para minimizar el acceso a memoria global
y utilizar la memoria compartida como una cache intermedia entre la lectura - operaci\'on -
escritura. El esquema b\'asico ser\'ia :

\begin{enumerate}
\item Cargar los datos de memoria global a memoria local.
\item Sincronizar todas las threads del bloque de tal modo que cada thread pueda
acceder a la memoria cargada por otra thread de forma segura.
\item Procesar los datos en memoria compartida.
\item Sincronizar nuevamente, si es necesario, para asegurar que todas las thread
terminaron de procesar los datos.
\item Escribir los resultados nuevamente a memoria global.
\end{enumerate}

Otro punto que mejora el rendimiento es seguir los patrones de accesos \'optimos a memoria.
Cada memoria tiene sus propias caracter\'isticas.

La memoria global reside en memoria del dispositivo, esta memoria es accedida
a trav\'es de transacciones de 16, 32 y 64 bytes. Dichas transacciones est\'an
alineadas. Cuando una warp ejecuta una instrucci\'on que accede a memoria global, 
esta genera las cantidad de transacciones necesarias dependiendo del tama\~no de dato
accedido de tal manera de poder satisfacer cada hilo y luego lo distribuye entre ellos.
Por lo general, mientras m\'as transacciones sean necesarias, m\'as datos innecesarios
son transferidos al warp y luego desechados, empeorando el rendimiento. Por ello
es importante que las instrucciones de acceso a memoria global sean hechas de tal
forma que los datos necesarios por los hilos est\'en los m\'as juntos posibles.

El accesos a memoria local solo ocurren para algunas variables autom\'aticas las cuales
son ubicadas en este espacio de memoria por el compilador. El espacio de memoria local
reside en memoria de dispositivo, por lo tanto su accesos tiene alta latencia y bajo
ancho de banda. Adem\'as est\'an sujetas a los mismo requerimientos de accesos que lo
nombrado anteriormente en el acceso a memoria global. Ya que el acceso a esta memoria
est\'a controlada por el compilador, este se encarga de generar los patrones de acceso
que maximicen el rendimiento.

La memoria compartida reside en los multiprocesadores. Por ello el accesos a esta memoria
tiene m\'as baja latencia y m\'as alto ancho de banda que la memoria local y la memoria global.
Para maximizar el ancho de banda, la memoria compartida es dividida en m\'odulos de
igual tama\~no, llamados bancos, los cuales pueden ser accedidos simult\'aneamente. Cualquier
requerimiento de lectura o escritura realizado a $n$ direcciones que caen en $n$ bancos
de memoria distintos pueden ser servidos simult\'aneamente. Del mismo modo, accesos simult\'aneos
de varios hilos a posiciones distintas del mismo banco generan la serializaci\'on del acceso.
Es importante destacar que si varios hilos acceden a la misma posici\'on de memoria, el
warp realiza una sola transacci\'on y luego distribuye la informaci\'on a todos los
hilos que la requirieron.

La memoria constante y memoria de textura son memorias que residen en memoria de
dispositivo, pero no analizaremos su patr\'on de acceso ya que este trabajo no hace
uso de este tipo de memorias.
